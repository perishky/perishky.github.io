<!DOCTYPE html>
<html>
  <head>
    <title>How to enter the tidyverse of madness</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="../../css/my-remark.css">
  </head>
  <body>
    <textarea id="source">

count: false
class: left, bottom

# How to enter the tidyverse of madness
&nbsp;
### Matthew Suderman
#### Senior Lecturer in Epigenetic Epidemiology

&lt;img src="IEU-logo-colour.png" width="50%"&gt;
---
layout: true

.logo[.mrcieu[
MRC Integrative Epidemiology Unit&nbsp;&nbsp;&nbsp;&nbsp;
]]

---

## What's wrong with R?

R is old (circa 1993). 

It was originally designed for doing **statistics**.

Since then, it has become one of the most popular tools for 
a relatively new discipline called **[data science](https://en.wikipedia.org/wiki/Data_science)**.

> "Data science is an interdisciplinary field that uses 
> scientific methods, processes, algorithms and systems 
> to extract knowledge and insights from 
> noisy, structured and unstructured data, 
> and apply knowledge and actionable insights
> from data across a broad range of application domains."
> https://en.wikipedia.org/wiki/Data_science

--

For data science, R has some critical short-comings:

1. Lacks **consistency**
2. Often **suprising**
3. Tends to be **slow** and **memory-intensive**
4. Built around **vectors and matrices** 
5. Functional language that can be **awkward** to use 
6. Error messages often **unhelpful**

---

## 1. Lacks **consistency**

.ii[
**function naming**

```
names, colnames
row.names, rownames
rowSums, rowsum
rowMeans, (no parallel rowmean exists)
browseURL, contrib.url, fixup.package.URLs
package.contents, packageStatus
getMethod, getS3method
read.csv and write.csv, load and save, readRDS and saveRDS
Sys.time, system.time
```
[https://r4stats.com/articles/why-r-is-hard-to-learn/]

**multiple ways to do things**

e.g. two different object-oriented systems: S3 and S4 systems
]

.ii[
**missing values**

```{r}
> x <- c(1,2,3,4,NA)
> y <- c(1.1,2.2,3.3,4.4,5.5)
> quantile(x)
Error in quantile.default(x) :
  missing values and NaN's not allowed if 'na.rm' is FALSE
> fit <- lm(y ~ x)
> length(x)
[1] 5
> length(residuals(fit))
[1] 4
```
]

---

## 2. Often **surprising** 

R often converts between data types behind the scenes, 
e.g. characters to factors, matrices to vectors, numeric to characters

```{r}
> x <- matrix(1:12,ncol=3)
> x
     [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12
```
--
```{r}
> class(x[,1:3])
[1] "matrix" "array"
```
--
```{r}
> class(x[,3]) ## what is this?
```
--
```{r}
[1] "integer"
```

---

```{r}
> x <- matrix(1:12,ncol=3)
> x
     [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12
```
--
```{r}
> x[1,1]
1
```
--
```{r}
> x[1,3] <- "a"
```
--
```{r}
> x[1,1] ## what is this?
```
--
```{r}
[1] "1"
```
---
```{r}
> y <- c(a=1,b=2,c=3)
> x <- data.frame(m=c("a","c"),n=1:2)
> x
  m n
1 a 1
2 c 2
> y
a b c
1 2 3
```
--
```{r}
> y[c("a","c")]
a c
1 3
```
--
```{r}
> y[x$m] ## what is this?
```
--
```{r}
a b
1 2
## at least it was until R v4.0
```

---

## 3. Tends to be **slow** and **memory-intensive**



---

## 4. Built around **vectors and matrices** 

whereas data science analyses **data frames**

---

## 5. Functional language that can be **awkward** to use 


---

## 6. Error messages often **unhelpful**


---


## tidyverse

.ii[
[Tidyverse](https://www.tidyverse.org/) is a collection of R packages
that aim to make the analysis of large datasets more convenient.

&lt;img src="tidyverse.png" width="75%"&gt;
]
.gap[&nbsp;]
.ii[
&lt;img src="data-science.png" width="100%"&gt;

- Data is **import**ed and prepared for analysis (made **tidy**).
- Analysis involves *transform**ing the data 
to answer specific questions. 
- Transformed data is **visualised** to discover relationships
and otherwise interpret the data. 
- **Models** are then applied to the data to evaluate 
expected or hypothesized relationships.
- Finally, conclusions are **communicate**d
not only to explain conclusions, but also illustrate 
how conclusions are supported by the data.
]
---

## tidyverse book

https://r4ds.had.co.nz/<br>
&lt;img src="tidyverse-book.png" width="100%"&gt;

---

## Using tidyverse packages

Installing all tidyverse packages.
```{r}
install.packages("tidyverse")
```

Loading all tidyverse packages.
```{r}
library(tidyverse)
```

---

## Our dataset

.ii[
In this session, we will explore the tidyverse by performing a basic data analysis
of a simulated dataset from a randomized control trial.

&lt;img src="blinking-guy.jpg" width="90%"&gt;
]
.gap[&nbsp;]
.ii[
**Background** Studies have shown that blink rate is positively associated with disengaged attention.
A drug has been developed that is known to reduce blink rate. 

**Aim** Determine if the drug will reduce attention deficits by reducing blink rate.

**Methods**

The were 4 arms: 

&nbsp; controls (A1), low dose (A2), medium (A3), high (A4)

Each arm had 100 randomly selected individuals.

Data was collected in 3 waves:

&nbsp; baseline (T0), 6 months (T1), 12 months (T2)
]
---

## Dataset schema

&lt;img src="datamodel.png" width="90%"&gt;

---

## Import the dataset

CSV files can be imported into R using `readr::read_csv()`.

```{r}
> data.dir <- "https://raw.githubusercontent.com/perishky/perishky.github.io/master/r/enter-the-tidyverse"
> arms <- read_csv(file.path(data.dir, "arms.csv"))
> participants <- read_csv(file.path(data.dir, "participants.csv"))
> visits <- read_csv(file.path(data.dir, "visits.csv"))
```

Files can be read locally from your computer or from a website online. 
Here we read the file from my github repository.

--

```{r}
> arms <- read_csv("arms.csv")
Rows: 4 Columns: 2
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
chr (1): arm
dbl (1): dose

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

> arms
# A tibble: 4 × 2
  arm    dose
  <chr> <dbl>
1 arm3      2
2 arm4      4
3 arm2      1
4 arm1      0
```

---

## Why use `read_csv` rather than `read.csv`

**Speed** `read_csv` is typically about 10x faster.  If this isn't fast enough, `data.table::fread` is even faster! 

--

**Better communication** `read_csv` provides more information about what it has imported into R and clearer error messages if the import failed via the `problems()` function.

```{r}
> arms <- read_csv("arms.csv")
Rows: 4 Columns: 2
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
chr (1): arm
dbl (1): dose
```

--

**Tidy format** `read_csv` creates tibbles rather than data frames.
These are more reproducible because they *do not* inherit behaviour from your specific system
so they will always produce the same results.

```{r}
> arms
# A tibble: 4 × 2
  arm    dose
  <chr> <dbl>
1 arm3      2
2 arm4      4
3 arm2      1
4 arm1      0
```

---

## Tidy

Tidy data is stored in tables called 'tibbles'.

1. "Each variable must have its own column."
2. "Each observation must have its own row."
3. "Each value must have its own cell."

--

> *Why use a tibble rather than a data frame?* (after all, tibbles are  data frames)
> 
> **Predictability** `tibble` never changes the types of inputs, e.g. character strings are never converted to factors.  It never changes column names.  It never adds row names.
> 
> **Convenience** Whereas `tibble` can create data frames column-by-column, there is a `tribble` function to create data frames row-by-row.
> 
> **Pretty printing** Typing the name of a data frame in R and pressing enter will print the entire data frame to the screen, regardless of its length or width! Tibbles will only show the first few rows and columns along with the dimensions of the data frame and data type of each column. 

---

## Creating tibbles

Tibbles can be created by column (just like a data frame)
```{r,eval=F}
> tibble(
  x=c("a","b"),
  y=c(3,1),
  z=c(pi,2*pi)
)
```
--
or by row like they'd be written in a data file
```{r}
> tribble(
  ~x, ~y, ~z,
  #--|--|----
  "a", 3, pi,
  "b", 1,2*pi
)
```

---

## Answering questions

How many participants are in each study arm?

--

.ii[
We'll use the `participants` tibble.

```{r}
> participants
# A tibble: 400 × 4
   participant   age sex   arm
         <dbl> <dbl> <chr> <chr>
 1           1  23.6 F     arm3
 2           2  24.7 M     arm4
 3           3  25.4 F     arm2
 4           4  25.0 F     arm2
 5           5  23.4 M     arm4
 6           6  25.9 M     arm2
 7           7  26.5 M     arm4
 8           8  26.6 F     arm1
 9           9  25.3 F     arm2
10          10  25.3 F     arm3
# … with 390 more rows
```
]

.gap[&nbsp;]

--


.ii[
We can use the `count(<tibble>,<column1>,<column2>,...)` function for this.
It is very similar to the `table` function in base R.

```{r}
> count(participants, arm)
# A tibble: 4 × 2
   arm      n
 <chr>  <int>
1 arm1    100
2 arm2    100
3 arm3    100
4 arm4    100
```
]

---

## Challenge!

See if you can count the number of female participants in each arm.

--

```{r}
> count(participants, arm, sex)
# A tibble: 8 × 3
  arm   sex       n
  <chr> <chr> <int>
1 arm1  F        48
2 arm1  M        52
3 arm2  F        43
4 arm2  M        57
5 arm3  F        50
6 arm3  M        50
7 arm4  F        46
8 arm4  M        54
```

---

## Filtering

That last output gives more information than we wanted. 
To focus on just females, 
we'll need to filter out information about males.

To do this, we use the `filter(<tibble>,<criteria>)` function.

```{r}
> counts <- count(participants, arm, sex)
> filter(counts, sex == "F")
# A tibble: 4 × 3
  arm   sex       n
  <chr> <chr> <int>
1 arm1  F        48
2 arm2  F        43
3 arm3  F        50
4 arm4  F        46
```

---

## Challenge!

See if you can obtain the same result by 

1. first applying the `filter` function to `participants` to remove males

2. and then applying the `count` function to the result.

--

```{r}
> females <- filter(participants, sex=="F")
> count(females, arm, sex)
# A tibble: 4 × 3
  arm   sex       n
  <chr> <chr> <int>
1 arm1  F        48
2 arm2  F        43
3 arm3  F        50
4 arm4  F        46
```

---

## Intermediate variables

For longer queries, it can get a little tedious creating variable names for each step. 

In the queries above, we had to create `counts` and `females` even though 
we aren't directly interested in those variables, e.g.

```{r}
> counts <- count(participants, arm, sex)
> filter(counts, sex == "F")
```

--

One solution is called **function composition**. 
```{r}
> filter(count(participants, arm, sex), sex=="F")
```

--

## Challenge!

See if you can obtain the same result with `count` as the outer function, 
i.e. `count(filter(...), ...)`

--

```{r}
> count(filter(participants, sex=="F"), arm, sex)
```

---

## Function composition gets extreme

Although function composition avoids having to name intermediate variables, 
they can make the code difficult to read.

For example, the following command summarizes information about each study arm
(we'll discuss the functions used here later).

```{r}
> summarize(group_by(left_join(filter(visits, wave=="wave2"), participants, by="participant"),arm,sex),n=n(), age=mean(age), bmi=mean(bmi), blink=mean(blink))
```

--
To be fair, we can split it across lines to make it a little easier to read.
```{r}
> summarize(
    group_by(
     left_join(
       filter(visits, wave=="wave2"), 
       participants, 
       by="participant"), 
      arm, 
      sex), 
    n=n(), 
    age=mean(age), 
    bmi=mean(bmi), 
    blink=mean(blink))
```

---

## Pipes

The "pipe" operator (`%>%`) was invented to solve this problem. 

This command ...

```{r}
> filter(count(participants, arm, sex), sex=="F")
```

--

... can be written like this

```{r}
> participants %>% filter(sex=="F") %>% count(arm, sex)
```

--

... or even better, split across multiple lines

```{r}
> participants %>%
  filter(sex=="F") %>% 
  count(arm, sex)
```

Notice how the first argument of each function is just output of the previous function
in the sequence.

---

## Challenge!

See if you can rewrite the following using pipes:

```{r}
> filter(count(participants,arm,sex), sex=="F")
```

--

```{r}
> participants %>% 
  count(arm,sex) %>% 
  filter(sex=="F")
```

---

## Challenge!

Now see if you can rewrite this command:
```{r}
> summarize(
    group_by(
     left_join(
       filter(visits, wave=="wave2"), 
       participants, 
       by="participant"), 
      arm, 
      sex), 
    n=n(), 
    age=mean(age), 
    bmi=mean(bmi), 
    blink=mean(blink))
```

--

```{r}
> visits %>%
  filter(wave=="wave2") %>%
  left_join(participants, by="participant") %>%
  group_by(arm,sex) %>%
  summarize(n=n(),age=mean(age), bmi=mean(bmi), blink=mean(blink))
```

---

## Calculating data summaries

The `summarise(<tibble>,<summary1>,<summary2>,...)` summarizes the columns of a tibble.

For example, we can use it to calculate
1. the average participant age, and 
2. how many of them are females.

--

```{r}
> summarise(participants, age=median(age), nfemales=sum(sex=="F"))
# A tibble: 1 × 2
    age nfemales
  <dbl>    <int>
1  25.0      187
```

Here is what it looks like using pipes.
```{r}
> participants %>% 
  summarise(age=median(age), nfemales=sum(sex=="F"))
```

---

## Calculating summaries of subsets

The previous example above provided summaries across *all* rows. 

It is also possible to summarize subsets of rows using `group_by(<tibble>,<groupby1>,<groupby2>,...)`.

For example, we can calculate, *for each arm of the study*
1. the average participant age, and 
2. the number of males and females.

```{r}
> participants %>% 
  group_by(arm,sex) %>%
  summarise(n=n(), age=mean(age))
```

--

You may have noticed that there is some magic going on here. 

1. `summarise` magically knows when a tibble has come from `group_by` and that it should summarize by the resulting groups.

2. The `n` function magically knows about the tibble subsets being processed by `summarise`.

---

## Challenge!

Can you think of an alternative to the mysterious `n` function?  (*this is a tricky question*)

```{r}
> participants %>% 
  group_by(arm,sex) %>%
  summarise(n=n(), age=mean(age))
```

--

*Hint:* Use the `length` function ...

--

```{r}
> participants %>% 
  group_by(arm,sex) %>%
  summarise(n=length(age), age=mean(age))
```

---

## .........................................................................................


The "visits" data frame tells us additional information about the participants 
from each visit, including the base line visit "wave0".
```{r}
visits
```
We'd like to augment the summary we gave above with the other baseline measurements 
for each study participant. 

To do this, we'll first need to:
1. filter "visits" to obtain baseline measurements
2. merge the result with "participants"
3. group the merged result by arm and sex
4. summarise the variables in group

```{r}
visits %>%
  filter(wave=="wave0") %>%
  left_join(participants,by="participant") %>%
  group_by(arm,sex) %>%
  summarise(n=n(), age=mean(age), bmi=mean(bmi), blink=mean(blink))
```
This isn't all that interesting because we're only looking at baseline numbers
for each study arm. Each arm should be roughly identical at baseline.
To see differences, we'd have to switch to the final wave.
```{r}
visits %>%
  filter(wave=="wave2") %>%
  left_join(participants,by="participant") %>%
  group_by(arm,sex) %>%
  summarise(n=n(), age=mean(age), bmi=mean(bmi), blink=mean(blink))
```
From this viewpoint, we can see that the arms do appear to 
differ with respect to blink rate. To remind ourselves the treatment
within each arm, we'll add 'dose' to the output.
This requires that we merge in the 'arms' data frame as well.
```{r}
query <- visits %>%
  filter(wave=="wave2") %>%
  left_join(participants,by="participant") %>%
  left_join(arms,by="arm") %>%
  group_by(arm,sex) %>%
  summarise(dose=dose[1],n=n(), age=mean(age), bmi=mean(bmi), blink=mean(blink))
query
```
The 'arm' and 'dose' columns are a bit redundant, we just want to show 'dose'. 
For this, we can use the `select` function.
```{r}
query %>% 
  ungroup() %>%
  select(dose,sex,n,age,bmi,blink)
```
The `ungroup` command is necessary to turn 'off' grouping 
so that `select` can treat its input as a data frame without any grouping assumptions.

It's a bit tedious to type all the column names when we just want to remove one.
```{r}
query <- query %>%
  ungroup() %>%
  select(-arm)
query
```
With male rows next to female rows, it is difficult to see if there were any 
treatment effects on BMI. We'll reorder ("arrange") the rows 
by sex and then by dose ascending.
```{r}
query %>%
  arrange(sex,dose)
```
Now we can more easily see that BMI appears to be increasing with dose.

To run a statistical test to determine if treatment increases BMI, 
we'll need variables for BMI before *and* after treatment. 
We will use the following model:

BMI measured at wave 2 ~ BMI at baseline + dose + sex + age

We could this as follows:
1. Create a baseline subset of 'visits'
2. Create a wave 2 subset of 'visits'
3. Merge these two so we have a data frame with bmi before and after treatment
4. Merge this with 'participants' and 'arms' to get participant age, sex and dose
5. Fit the linear model and extract the coefficients and p-values

```{r}
baseline <- visits %>% filter(wave=="wave0") %>% select(participant, bmi)
wave2 <- visits %>% filter(wave=="wave2") %>% select(participant, bmi)
baseline %>%
  left_join(wave2, by="participant", suffix=c("", ".after")) %>%
  left_join(participants, by="participant") %>%
  left_join(arms, by="arm") %>%
  lm(bmi.after ~ bmi + dose + sex + age, data=.) %>%
  summary() %>%
  coef()
```
As expected, BMI as baseline is strongly correlated with BMI after treatment. 
Beyond that, age and sex have almost no effect. 
Treatment dose, however, does appear to have a role in increasing BMI.

The data transformation we just applied is called a 'pivot from long to wide'.
Initially, the data is 'long' with one row per BMI measurement per participant. 
After the pivot, the data is 'shorter' with one row per participant
but 'wider' due to having two columns providing BMI measurements.

Because pivots are so common, tidyr has defined two functions 
for performing pivots, `pivot_wider` and its inverse `pivot_longer`.
```{r}
visits %>% 
  filter(wave %in% c("wave0","wave2")) %>%
  pivot_wider(
    id_cols=participant,
    names_from=wave,
    values_from=bmi) %>%
  rename(bmi=wave0,bmi.after=wave2) %>%
  left_join(participants, by="participant") %>%
  left_join(arms, by="arm") %>%
  lm(bmi.after ~ bmi + dose + sex + age, data=.) %>%
  summary() %>%
  coef()
```

### Transforming sequences of data with `purrr`
Earlier we asked if treatment dose is associated with changes in BMI.
Now we ask if treatment dose is associated with changes in 
blink rate, BMI, HDL cholesterol, triglycerides and LDL cholesterol.

To do this, we first create a function for testing any single phenotype.
We use the code we developed above, but with three small changes.
```{r}
calculate.statistics <- function(phenotype) {
  visits %>% 
  filter(wave %in% c("wave0","wave2")) %>%
  pivot_wider(
    id_cols=participant,
    names_from=wave,
    values_from=phenotype) %>%                            ## 1st change
  rename(var.base=wave0,var.after=wave2) %>%              ## 2nd change
  left_join(participants, by="participant") %>%
  left_join(arms, by="arm") %>%
  lm(var.after ~ var.base + dose + sex + age, data=.) %>% ## 3rd change
  summary() %>%
  coef()
}
```
We can recalculate the statistics for BMI.
```{r}
calculate.statistics("bmi")
```
We could also calculate statistics for HDL cholesterol.
```{r}
calculate.statistics("hdl")
```

We're really only interested in the statistics related to 'dose'. 
These tell us how treatment dose relates to the phenotype.
We create another function that only returns those statistics.
```{r}
calculate.phenotype.statistics <- function(phenotype) {
  calculate.statistics(phenotype) %>%  # use the other function to calculate
    as.tibble(rownames="variable") %>% # convert the output from matrix to tibble
    filter(variable=="dose") %>%       # retain only the row for 'dose' statistics
    select(-variable) %>%              # remove the variable column
    mutate(Phenotype=phenotype) %>%    # add a column with the phenotype name
    relocate(Phenotype)                # make this the first column
}
```

```{r}
calculate.phenotype.statistics("bmi")
```
Now we use the `map` function to apply the function 
`calculate.phenotype.statistics' to each phenotype.
```{r}
list("blink","bmi","hdl","trig","ldl") %>%
  map(calculate.phenotype.statistics) %>%
  bind_rows()
```

## Visualizing data using `ggplot`

We've looked at a lot of tables of statistics.
For humans, it is often easier to identify relationships
through visualization than by looking at a lot of numbers.

Here we show how to plot the relationship between blink rate, 
treatment dose and time:
1. pipe the data to the plotting function `ggplot`
2. tell `ggplot` to separate blink rates by dose, and within dose by wave
3. tell `ggplot` that blink rate distributions will be displayed as box plots
4. label the x and y axes
```{r}
visits %>%
  left_join(participants,by="participant") %>%
  left_join(arms,by="arm") %>%
  ggplot(aes(x=factor(dose), y=blink, fill=wave)) + 
    geom_boxplot() + 
    xlab("treatment dose") + 
    ylab("blink rate") 
```
```{r}
visits %>%
  left_join(participants,by="participant") %>%
  left_join(arms,by="arm") %>%
ggplot(aes(x=factor(dose), y=hdl, fill=wave)) + 
  geom_boxplot() + 
  xlab("treatment dose") + 
  ylab("HDL cholesterol") 
```

## Model 

We'll leave this for another day.

See the book for more details: https://r4ds.had.co.nz/model-intro.html

## Communicate

We'll leave this for another day.

See the book for more details: https://r4ds.had.co.nz/communicate-intro.html

---

## Not everyone agrees

https://dplyr.tidyverse.org/articles/base.html

BUT, GitHub - matloff/TidyverseSkeptic: An opinionated view of the Tidyverse "dialect" of the R language.


1. tidyr needs more memorization (eg. dplyr has > 200 functions!)
often base R actually reduces how much you have to remember but provide more primitive commands that can be combined in different ways to do a wide variety of operations.  tidyr tends to provide specific commands for higher level operations so can require more memorization.
e.g. count the number of different command names in "one table verbs" and "two table verbs" for tidyr versus base R
    compare purrr vs. base-R example in tidyverseskeptic
2. tidyr is new and changing (https://dplyr.tidyverse.org/articles/compatibility.html) whereas base R is fairly static, what do you do when your code uses an old version of dplyr?
3. when tidyr functions fail (ggplot,dplyr functions) it's often really difficult to figure out why because they are so much more high-level
4. ggplot2 is actually not tidyverse!  it kind of looks like pipes but actually isn't
5. dplyr can be much slower than alternatives, e.g. data.table


see tables that match base R to tidyr functions

good challenges: 
this is how you do this in base R, how to do it with tidyr




---

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script src="slideshow.js"></script>
  </body>
</html>      
 

